{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-09T21:03:57.939090Z",
     "start_time": "2024-10-09T21:03:57.935751Z"
    }
   },
   "source": [
    "from csv import DictReader\n",
    "from functools import cached_property, cache\n",
    "\n",
    "\n",
    "class logger:  # noqa\n",
    "    log = staticmethod(print)  # noqa"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T21:03:58.014912Z",
     "start_time": "2024-10-09T21:03:57.984889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import doctest\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import defaultdict\n",
    "\n",
    "UNDEFINED = object()\n",
    "\n",
    "\n",
    "class AbstractSerialMapper(ABC):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._data = []\n",
    "\n",
    "    @abstractmethod\n",
    "    def add(self, value, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "\n",
    "class SerialUnidirectionalMapper(AbstractSerialMapper):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def add(self, data, *args, key=None, **kwargs):\n",
    "        if key is UNDEFINED or key is None:\n",
    "            self._data.append([data])\n",
    "        else:\n",
    "            self._data[key].append(data)\n",
    "\n",
    "\n",
    "class SerialBidirectionalMapper(AbstractSerialMapper):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._data = []\n",
    "        self._inverse_data = defaultdict(lambda: UNDEFINED)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self._data[key]\n",
    "\n",
    "    def add(self, value, *args, **kwargs):\n",
    "        self._inverse_data[value] = len(self._data)\n",
    "        self._data.append(value)\n",
    "\n",
    "    @property\n",
    "    def inverse(self):\n",
    "        return self._inverse_data\n",
    "\n",
    "\n",
    "class InMemory2DIndexer(object):\n",
    "    \"\"\"\n",
    "    An in-memory 2D indexer for efficiently mapping rows and columns from CSV data.\n",
    "\n",
    "    This class provides an efficient mechanism to map and store large datasets\n",
    "    using bijective mappings between row/column names and their corresponding IDs.\n",
    "\n",
    "    Attrs:\n",
    "        LIMIT_TO_LOAD_IN_MEMORY (int): Maximum number of lines to load into memory.\n",
    "        DEFAULT_CSV_DATA_INSTANTIATOR (callable): Default instantiator to use ...\n",
    "\n",
    "    Args:\n",
    "        row_name (str): The name representing the rows.\n",
    "        column_name (str): The name representing the columns.\n",
    "\n",
    "    Methods:\n",
    "        index_from_csv(file_path, row_header, column_header, exclude_columns, instantiator, limit, verbose):\n",
    "            Indexes data from a CSV file, mapping rows and columns by their headers.\n",
    "\n",
    "    Example usage:\n",
    "        >>> indexer1 = InMemory2DIndexer(row_name=\"ID\", column_name=\"Attribute\")\n",
    "        >>> indexer1._row_name\n",
    "        'ID'\n",
    "        >>> indexer1._column_name\n",
    "        'Attribute'\n",
    "\n",
    "        >>> indexer2 = InMemory2DIndexer(row_name=\"userId\", column_name=\"movieId\")\n",
    "        >>> # Testing with correct row and column headers\n",
    "        >>> indexer2.index_from_csv(\n",
    "        ...     file_path=\"./ml-32m/ratings.csv\",\n",
    "        ...     row_header=\"userId\",\n",
    "        ...     column_header=\"movieId\"\n",
    "        ... )  # # doctest:+ELLIPSIS\n",
    "        Limit of entries (.i.e 10) to load has been reached. Exiting without loading the rest...\n",
    "        <__main__.InMemory2DIndexer object at ...\n",
    "        >>> # Testing with incorrect row header\n",
    "        >>> indexer2.index_from_csv(\n",
    "        ...     file_path=\"./ml-32m/ratings.csv\",\n",
    "        ...     row_header=\"WrongHeader\",\n",
    "        ...     column_header=\"movieId\"\n",
    "        ... )\n",
    "        Traceback (most recent call last):\n",
    "          File \"/usr/lib/python3.11/doctest.py\", line 1355, in __run\n",
    "            exec(compile(example.source, filename, \"single\",\n",
    "          File \"<doctest __main__.InMemory2DIndexer[3]>\", line 1, in <module>\n",
    "            indexer.index_from_csv(\n",
    "          File \"/tmp/ipykernel_108131/2003057470.py\", line 129, in index_from_csv\n",
    "            row_header is None or row_header == self._row_name\n",
    "        AssertionError: Invalid row header provided: 'userId'. The row header must be either None or match the previously defined row header 'ID'.\n",
    "\n",
    "        >>> # Testing with correct row and column headers\n",
    "        >>> indexed_data2 = InMemory2DIndexer(row_name=\"userId\", column_name=\"movieId\")\n",
    "        >>> indexed_data2 = indexer.index_from_csv(\n",
    "        ...     file_path=\"./ml-32m/ratings.csv\",\n",
    "        ...     row_header=\"userId\",\n",
    "        ...     column_header=\"movieId\",\n",
    "        ...     limit=400\n",
    "        ... )  # Expected to pass without error\n",
    "        Limit of entries (.i.e 400) to load has been reached. Exiting without loading the rest...\n",
    "        <__main__.InMemory2DIndexer object at ...\n",
    "        >>> # Testing the content of the indexer\n",
    "        >>> len(indexed_data._row_to_id_bmap._data) == 5\n",
    "        True\n",
    "    \"\"\"\n",
    "\n",
    "    LIMIT_TO_LOAD_IN_MEMORY = 1_000_000_000_000\n",
    "\n",
    "    DEFAULT_CSV_DATA_INSTANTIATOR = lambda _cls, data, columns: tuple(\n",
    "        data[k] for k in (columns if columns else data)\n",
    "    )\n",
    "\n",
    "    def __init__(self, row_name: str, column_name: str):  # typing\n",
    "        self._row_name = row_name\n",
    "        self._column_name = column_name\n",
    "        self._column_to_id_bmap = SerialBidirectionalMapper() # bijective mapping\n",
    "        self._row_to_id_bmap = SerialBidirectionalMapper()\n",
    "        self._data_by_row_id = SerialUnidirectionalMapper() # subjective mapping\n",
    "        self._data_by_column_id = SerialUnidirectionalMapper()\n",
    "\n",
    "        # SOME PRIVATE SAFETY CHECK PROPERTIES\n",
    "        self.__is_indexed = False\n",
    "\n",
    "    def index_from_csv(\n",
    "        self,\n",
    "        *,\n",
    "        file_path: str,\n",
    "        row_header: str,\n",
    "        column_header: str,\n",
    "        data_columns=None,\n",
    "        instantiator=None,\n",
    "        limit=None,\n",
    "        verbose=False,\n",
    "    ):  # typing\n",
    "\n",
    "        assert (\n",
    "            row_header is None or row_header == self._row_name\n",
    "        ), f\"Invalid row header provided: '{row_header}'. The row header must be either None or match the previously defined row header '{self._row_name}'.\"\n",
    "        assert (\n",
    "            column_header is None or column_header == self._column_name\n",
    "        ), f\"Invalid column header provided: '{column_header}'. The column header must be either None or match the previously defined column header '{self._column_name}'.\"\n",
    "\n",
    "        instantiator = instantiator or self.DEFAULT_CSV_DATA_INSTANTIATOR\n",
    "        limit_to_load = limit or self.LIMIT_TO_LOAD_IN_MEMORY\n",
    "        indexed_count = 0\n",
    "\n",
    "        with open(file_path, mode=\"r\", newline=\"\") as csvfile:\n",
    "            for line in DictReader(csvfile):\n",
    "                indexed_count += 1\n",
    "                row, column = line[row_header], line[column_header]\n",
    "\n",
    "                row_id = self._row_to_id_bmap.inverse[row]\n",
    "                column_id = self._column_to_id_bmap.inverse[column]\n",
    "\n",
    "                if row_id is UNDEFINED:\n",
    "                    # This row is a new one, so add it\n",
    "                    self._row_to_id_bmap.add(row)\n",
    "\n",
    "                if column_id is UNDEFINED:\n",
    "                    # This column is a new one, so add it\n",
    "                    self._column_to_id_bmap.add(column)\n",
    "\n",
    "                # Add the data without the useless columns for indexing\n",
    "                data = instantiator(line, data_columns)\n",
    "\n",
    "                self._data_by_row_id.add(data=data, at=row_id)\n",
    "                self._data_by_column_id.add(data=data, at=column_id)\n",
    "\n",
    "                if verbose:\n",
    "                    logger.log(\n",
    "                        f\"Indexed the line {indexed_count} of {file_path} successfully\"\n",
    "                    )\n",
    "\n",
    "                if indexed_count == limit_to_load:\n",
    "                    logger.log(\n",
    "                        f\"Limit of entries (.i.e {limit_to_load}) to load has been reached. Exiting without loading the rest... \"\n",
    "                    )\n",
    "                    break\n",
    "        self.__is_indexed = True\n",
    "        return self  # To enable method chaining (Fluent pattern)\n",
    "\n",
    "    @cached_property\n",
    "    def matrix(self):  # typing\n",
    "        if not self.__is_indexed:\n",
    "            raise AssertionError(\n",
    "                \"Cannot define matrix if the indexer is not indexed, ensure you called `index_from_csv()`\"\n",
    "            )\n",
    "        return [\n",
    "            [1 for movie_id in range(indexed_data.columns_count)]\n",
    "            for user_id in range(indexed_data.rows_count)\n",
    "        ]\n",
    "\n",
    "    @cached_property\n",
    "    def numpy_matrix(self):  # typing\n",
    "        import numpy as np\n",
    "\n",
    "        return np.array(self.matrix)\n",
    "\n",
    "    @property\n",
    "    def rows_count(self) -> int:\n",
    "        return len(self._row_to_id_bmap)\n",
    "\n",
    "    @property\n",
    "    def columns_count(self) -> int:\n",
    "        return len(self._column_to_id_bmap)\n",
    "\n",
    "\n",
    "# FIXME: Replace it by proper unit tests at the end\n",
    "# doctest.testmod() # Skip this tests for the moment\n",
    "indexed_data = InMemory2DIndexer(\n",
    "    row_name=\"userId\", column_name=\"movieId\"\n",
    ").index_from_csv(\n",
    "    file_path=\"./ml-32m/ratings.csv\", row_header=\"userId\", column_header=\"movieId\", limit=400\n",
    ")\n",
    "\n",
    "# doctest.testmod()\n",
    "# indexed_data.find_row_by_row_id()\n",
    "# indexed_data.find_column_by_colum_id()\n",
    "# indexed_data.find_column_by_id()"
   ],
   "id": "871f50e90a230ae8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit of entries (.i.e 400) to load has been reached. Exiting without loading the rest... \n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T21:04:02.073067Z",
     "start_time": "2024-10-09T21:04:02.068036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# From here, we will use the domain vocabulary\n",
    "data_matrix = indexed_data.matrix\n",
    "data_np_matrix = indexed_data.numpy_matrix\n",
    "indexed_data.rows_count, indexed_data.columns_count\n"
   ],
   "id": "e1d05f45c4222908",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 327)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
